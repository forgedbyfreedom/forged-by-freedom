"""
üåê Full OpenAI ‚Üí Pinecone Sync (chunk-safe version)
--------------------------------------------------
Automatically syncs all transcript files in /transcripts
to your Pinecone vector index, splitting large files
to stay within OpenAI model limits.
"""

import os
import glob
from tqdm import tqdm
import openai
import pinecone

# === Environment variables ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

if not OPENAI_API_KEY or not PINECONE_API_KEY:
    raise RuntimeError("‚ùå Missing required environment variables (OpenAI or Pinecone key).")

# === OpenAI & Pinecone init ===
openai.api_key = OPENAI_API_KEY
pinecone.init(api_key=PINECONE_API_KEY)

INDEX_NAME = "forged-freedom-ai"

# Create index if it doesn‚Äôt exist
existing_indexes = [i.name for i in pinecone.list_indexes()]
if INDEX_NAME not in existing_indexes:
    print(f"ü™∂ Creating Pinecone index: {INDEX_NAME}")
    pinecone.create_index(INDEX_NAME, dimension=3072)

index = pinecone.Index(INDEX_NAME)

# === File discovery ===
transcript_files = sorted(glob.glob("transcripts/*.txt"))
print(f"üìö Found {len(transcript_files)} transcript files to sync")

# === Helper: Split long text into safe chunks ===
def chunk_text(text: str, max_chars: int = 12000):
    """Split text into OpenAI-safe chunks."""
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]

# === Main upload loop ===
for filepath in tqdm(transcript_files, desc="Uploading transcripts"):
    filename = os.path.basename(filepath)
    with open(filepath, "r", encoding="utf-8") as f:
        text = f.read().strip()

    chunks = chunk_text(text)
    vectors = []

    for i, chunk in enumerate(chunks):
        try:
            # Create embeddings safely
            response = openai.embeddings.create(
                model="text-embedding-3-large",
                input=chunk
            )
            embedding = response.data[0].embedding
            vectors.append({
                "id": f"{filename}-{i}",
                "values": embedding,
                "metadata": {"source": filename, "chunk": i}
            })
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping chunk {i} from {filename}: {e}")

    if vectors:
        index.upsert(vectors=vectors)
        print(f"‚úÖ Uploaded {len(vectors)} chunks from {filename}")

print("üéâ All transcripts uploaded successfully to Pinecone!")

#!/usr/bin/env python3
"""
ğŸ§  Ultimate Transcript Consolidation + Upload Script
Version: November 2025
Author: ForgedByFreedom

âœ… Consolidates all scattered transcript folders (@channel style)
âœ… Deduplicates identical transcript files
âœ… Builds fresh master_transcript1.txt for each channel
âœ… Uploads all masters to OpenAI Assistant storage (no duplicates)
âœ… Removes old 'metadata' argument issue
"""

import os
import hashlib
from glob import glob
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --------------------------
# Utility Functions
# --------------------------
def file_md5(filepath):
    """Return MD5 hash of file contents."""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


# --------------------------
# Step 1: Consolidate & Deduplicate
# --------------------------
def consolidate_transcripts(root_dir):
    print(f"\nğŸ“‚ Scanning for transcript folders in: {root_dir}")
    channel_dirs = glob(os.path.join(root_dir, "**", "@*"), recursive=True)
    print(f"ğŸ” Found {len(channel_dirs)} potential channel folders.\n")

    seen_hashes = set()
    cleaned_folders = []

    for channel_dir in channel_dirs:
        if not os.path.isdir(channel_dir):
            continue

        txt_files = [
            f for f in glob(os.path.join(channel_dir, "*.txt"))
            if not f.endswith("master_transcript1.txt")
        ]
        if not txt_files:
            continue

        combined_path = os.path.join(channel_dir, "master_transcript1.txt")

        with open(combined_path, "w", encoding="utf-8") as master:
            for txt_file in sorted(txt_files):
                with open(txt_file, "r", encoding="utf-8", errors="ignore") as f:
                    contents = f.read().strip()
                    if not contents:
                        continue
                    md5 = hashlib.md5(contents.encode("utf-8")).hexdigest()
                    if md5 not in seen_hashes:
                        seen_hashes.add(md5)
                        master.write(contents + "\n\n")
                    else:
                        print(f"ğŸ§¹ Skipped duplicate file: {txt_file}")

        cleaned_folders.append(channel_dir)
        print(f"âœ… Built clean master transcript: {combined_path}")

    print(f"\nğŸ¯ Consolidation complete â€” {len(cleaned_folders)} folders processed.\n")
    return cleaned_folders


# --------------------------
# Step 2: Upload to OpenAI
# --------------------------
def upload_transcripts(cleaned_folders):
    uploaded_count = 0
    seen_filenames = set()

    print("ğŸ“¤ Uploading consolidated transcripts to OpenAI...\n")

    for folder in cleaned_folders:
        master_path = os.path.join(folder, "master_transcript1.txt")
        if not os.path.exists(master_path):
            continue

        filename = os.path.basename(folder) + "_master.txt"
        if filename in seen_filenames:
            print(f"âš ï¸ Skipping duplicate upload: {filename}")
            continue
        seen_filenames.add(filename)

        try:
            with open(master_path, "rb") as f:
                client.files.create(file=f, purpose="assistants")
            print(f"âœ… Uploaded: {master_path}")
            uploaded_count += 1
        except Exception as e:
            print(f"âŒ Error uploading {master_path}: {e}")

    print(f"\nğŸš€ Upload complete â€” {uploaded_count} transcripts uploaded to OpenAI.\n")


# --------------------------
# Step 3: Full Pipeline Runner
# --------------------------
def main():
    print("\nğŸ§  Starting full transcript cleanup + upload pipeline...\n")

    repo_root = os.getcwd()

    # Step 1: Consolidate and deduplicate transcripts
    cleaned_folders = consolidate_transcripts(repo_root)

    # Step 2: Upload everything cleanly
    upload_transcripts(cleaned_folders)

    print("âœ… All done â€” all transcripts are now deduplicated, cleaned, and uploaded.\n")


if __name__ == "__main__":
    main()

